Берём заголовок статьи. Переводим каждое слово заголовка в 32-мерный вектор при помощие word2vec. Допускаем, что длина заголовка фиксирована и оставляет 20 слов. Применяем операция flatten на 20-мерный вектор векторов слов, получаем 640-мерный вектор. В результате, для заголовка статьи получили вектор. Теперь надо отобразить вектора заголовков в другое пространство, так чтобы расстояния между новыми векторами имели семантическое значение. Наверное, для этого может использоваться алгоритм, похожий на word embedding. Рассматриваем в качестве аналога предложения список источников и саму статью. Подстраиваем вектора так, чтобы определить вероятность встретить статью с таким списком источников. В результате получаем вектора заголовков и можем провести кластеризацию публикаций на основе расстояний между векторами.

Граф цитирований может быть построен при помощи библиотеки https://pybliometrics.readthedocs.io/en/stable/classes/AbstractRetrieval.html

TODO: как работает word embedding

Возможные проблемы. В word embedding корпус слов имеет ограниченный размер, в то время как корпус статей постоянно расширяется. Слово используется много раз, статья используется только если ее цитируют.